---
title: "Evalauting correlation between t-statistics"
subtitle: 'Sub-sampling data'
author: "Developed by [Gabriel Hoffman](http://gabrielhoffman.github.io/)"
date: "Run on `r Sys.time()`"
documentclass: article
output: 
  html_document:
  toc: true
  smart: false
vignette: >
  %\VignetteIndexEntry{Decorrelate}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\usepackage[utf8]{inputenc}
---


<!--- 

 rmarkdown::render("corr_btw_tstats.Rmd");


--->

  


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning=FALSE,
  message=FALSE,
  error = FALSE,
  tidy = FALSE,
  dev = c("png", "pdf"),
  cache = TRUE,
  package.startup.message = FALSE,
  cache.lazy = FALSE)
```

```{r load.packages, cache=FALSE}
library(MASS)
library(ggplot2)
library(Rfast)
library(variancePartition)
library(tidyverse)
library(broom)
library(heivr)
library(BiocParallel)
library(deming)
library(mcr)
```

```{r test}
heivr_se = function(x,y, v_x, v_y, tol=1e-3){

  res = heivr(x, y, v_x, v_y, tol=tol)
  res_null = heivr(x, y, v_x, v_y, tol=tol, null_model=TRUE)

  LR = -2*(res_null$logLik - res$logLik)

  f = function(se){
    (pnorm( res$rho, 0, se, lower.tail=FALSE, log=TRUE) - pchisq(LR, 1, lower.tail=FALSE, log=TRUE))^2
  }

  obj = optimize(f, interval=c(1e-4, 4))
  se = obj$minimum

  se
}

cov.se = function(x,y, nboot=1000){

  stopifnot( length(x) == length(y))
  
  ## bootstrap out
  bs.out <- replicate(nboot, {
    i <- sample.int(length(x), length(x), replace=TRUE)
    cov(x[i], y[i])
  })
  sd(bs.out)
}

heivr.boot.se = function(x, y, v.x, v.y, nboot=1000){

  stopifnot( length(x) == length(y))
  
  ## bootstrap out
  bs.out <- replicate(nboot, {
    i <- sample.int(length(x), length(x), replace=TRUE)
    heivr(x[i], y[i], v.x[i], v.y[i])$rho
  })
  sd(bs.out)
}
```

```{r sims.function, cache=FALSE}
run_simulation = function(nmax, ngenes, cor.target, nvalues, hsq){

  # simulate fixed covariate
  info = data.frame(x = rnorm(nmax))
  rownames(info) = paste0("s", 1:nmax)

  # simulate coefficients
  if( cor.target == 1 ){
    beta = rnorm(ngenes, 3)
    Beta = cbind(beta, beta)
  }else{
    # Sigma = diag(1,2)
    Sigma = diag(c(5,1.4))
    Sigma[1,2] = cor.target * prod(sqrt(diag(Sigma)))
    Sigma[2,1] = Sigma[1,2]
    Beta = rmvnorm(ngenes, c(1,-1.5), Sigma)
  }
  rownames(Beta) = paste0("gene_", 1:nrow(Beta))

  # Simulate gene expression
  geneExpr1 = t(sapply(Beta[,1], function(b){
    eta = info$x*b 

    # set cor(y, eta)^2 = hsq
    se = sqrt((1-hsq) / hsq * var(eta))
    eta + rnorm(nmax, 0, se)
    }))
  colnames(geneExpr1) = paste0("s", 1:nmax)

  geneExpr2 = t(sapply(Beta[,2], function(b){
    eta = info$x*b 

    # set cor(y, eta)^2 = hsq
    se = sqrt((1-hsq) / hsq * var(eta))
    eta + rnorm(nmax, 0, se)
    }))
  colnames(geneExpr2) = paste0("s", 1:nmax)

  df = lapply( nvalues, function(N){

    message("\r", N, '      ', appendLF=FALSE)

    # subset data, fit regression models, and extract results
    idx1 = sample.int(nmax, N, replace=FALSE)
    fit1 = dream(geneExpr1[,idx1], ~ x, info[idx1,,drop=FALSE], quiet=TRUE)
    fit1 = eBayes(fit1)
    tab1 = topTable(fit1, coef="x", number=Inf, sort.by='none')

    idx2 = sample.int(nmax, N, replace=FALSE)
    fit2 = dream(geneExpr2[,idx2], ~ x, info[idx2,,drop=FALSE], quiet=TRUE)
    fit2 = eBayes(fit2)
    tab2 = topTable(fit2, coef="x", number=Inf, sort.by='none')

    # merge and compute correlation between t-stats
    tab = merge(tab1, tab2, by="row.names")

    rm(fit1, tab1, fit2, tab2)

    # correlation
    # with(tab, cor.test(logFC.x, logFC.y)) %>% 
    #   tidy %>%
    #   mutate(se = sqrt((1 - estimate^2)/parameter), N = N)

    # res = cov.wt(with(tab, cbind(logFC.x, logFC.y)), sqrt(with(tab, (logFC.x/t.x)^2 + (logFC.y / t.y)^2)), cor=TRUE)

    a = with(tab, cor.test(logFC.x, logFC.y)) %>% tidy %>% mutate(se = sqrt((1 - estimate^2)/parameter))

    # Deming
    # fit = deming(logFC.y ~ logFC.x, data=tab, xstd = logFC.x / t.x, ystd = logFC.y / t.y, jackknife=FALSE)

    # # a = with(tab, sqrt(var(logFC.x) - mean((logFC.x / t.x)^2)))
    # # b = with(tab, sqrt(var(logFC.y) - mean((logFC.y / t.y)^2)))

    # s = with(tab, sd(logFC.x) / sd(logFC.y))
    # cor.deming = fit$coefficients[2] * s
    # cor.deming.se = fit$se[2] * s

    # # mcr
    # er = with(tab, mean((logFC.x / t.x)^2) / mean((logFC.y / t.y)^2))
    # it = with(tab, mcreg(logFC.x, logFC.y, error.ratio=er, method.reg="Deming"))
    # s = with(tab, sd(logFC.x) / sd(logFC.y))
    # cor.deming2 = it@para[2,1] * s
    # cor.deming2.se = it@para[2,2] * s

    hobj.mom = with(tab, heivr_mom(logFC.x, logFC.y, (logFC.x/t.x)^2, (logFC.y/t.y)^2, 40 ))

    # heivr
    hobj = with(tab, heivr(logFC.x, logFC.y, (logFC.x/t.x)^2,(logFC.y/t.y)^2))
    
    # covariance  
    data.frame( #cov = with(tab, cov(logFC.x, logFC.y)),
                #cov.se = with(tab, cov.se(logFC.x, logFC.y)),
                cor = with(tab, cor(logFC.x, logFC.y)), 
                cor.se = a$se,
                cor.mom = hobj.mom$rho,
                cor.mom.se = hobj.mom$rho.se,
                # cor.deming = cor.deming,
                # cor.deming.se = cor.deming.se,
                # cor.deming2 = cor.deming2,
                # cor.deming2.se = cor.deming2.se,
                cor.heivr = hobj$rho,
                cor.heivr.se = hobj$rho.se,
                N = N)
  })
  df = do.call(rbind, df)
  
  # combine results
  rbind(data.frame(Method = "Pearson", 
                  cor = df$cor, 
                  se = df$cor.se, 
                  N = df$N), 
        data.frame(Method = "MOM", 
                  cor = df$cor.mom, 
                  se = df$cor.mom.se, 
                  N = df$N), 
        # data.frame(Method = "Deming", 
        #           cor = df$cor.deming, 
        #           se = df$cor.deming.se,
        #           N = df$N), 
        # data.frame(Method = "Deming2", 
        #           cor = df$cor.deming2, 
        #           se = df$cor.deming2.se,
        #           N = df$N),
        data.frame(Method = "heivr", 
                  cor = df$cor.heivr, 
                  se = df$cor.heivr.se, 
                  N = df$N))
}
```

```{r run.sims}
# simulation parameters
nmax = 1000
ngenes = 1000
cor.target = .8
nvalues = c(seq(30, 100, by=30), seq(120, 500, by=75), seq(520, nmax, by=150))
hsq = 0.6
 
# df_param = expand.grid(hsq = c(.01, 0.05, 0.2), 
#             cor.target = c(0.2, .7, 1))

df_param = expand.grid(hsq = .02, cor.target = .8)
 
df = lapply( 1:nrow(df_param), function(i){

  df = run_simulation(nmax, ngenes, cor.target = df_param$cor.target[i], nvalues, hsq = df_param$hsq[i])
  df$cor.target = df_param$cor.target[i]
  df$hsq = df_param$hsq[i]
  df
}) 
df = do.call(rbind, df)
```







 rmarkdown::render("corr_btw_tstats.Rmd");


```{r plot1, cache=FALSE, fig.height=8, fig.width=12}
col = c(heivr = "red3", Pearson = "blue3", MOM="green3")
# , Deming = "green2", Deming2 = "orange", 
ggplot(df, aes(N, pmax(0, pmin(1, cor)), col=Method)) +
  geom_errorbar(aes(ymin = pmax(0,cor - 1.96*se), ymax = pmin(1, cor + 1.96*se)), width=0) +
  geom_point() +
  geom_hline(aes(yintercept=cor.target), linetype="dashed", color="black") +
  theme_classic() +
  theme(aspect.ratio=1) +
  scale_y_continuous(limits=c(0,1), expand=c(0,0.03)) +
  ylab("Estimated correlation") +
  xlab("Sample size") +
  scale_color_manual(values = col) + 
  facet_grid(cor.target ~ hsq)
```

Simulate two datasets with `r nmax` samples and `r format(ngenes, scientific=FALSE, big.mark=',')` genes where the logFC between datasets have a correlation of `r unique(df$cor.target)`.  Perform analysis on random subsets of the data, and evaluate the correlation between the estimated t-statistics.  For each gene, the variable of interest explains `r 100*unique(df$hsq)`% of the variance.

Method of moments error is simple and fast, but is not bounded between 0 and 1.  This is mostly a problem when measurement error is large.  Also, the MOM estimater has larger sampling variance than the MLE does. 

Cite Fuller (1987) on measurement error and MOM.  Deming proposes way to estimate coefficients, but not correlation.

Attentuation bias, or regression dilution. Originally recognized by Spearman (1904) [The proof and measurement of association between two things]







Dashed line is the simulated correlation between coefficients
```{r plot2, cache=FALSE, fig.height=6, fig.width=6, eval=FALSE, echo=FALSE}
perc = '%'
col = c(heivr = "red2", Deming = "green2", Pearson = "blue2")
main = bquote(N[max] == .(nmax) ~~~~ cor[target] == .(cor.target) ~~~~ h^2 == .(100*hsq)~.(perc))
plot(1, type='n', xlim=c(0, nmax), ylim=c(0,1), ylab="Estimated correlation between true effects", xlab="Sample size", main=main)

with(df, arrows(x0=N, y0=cor - 1.96*cor.se,
                x1=N, y1=cor + 1.96*cor.se, 
                col=col["Pearson"], lwd=2, code=1, angle=90, length=0))

with(df, arrows(x0=N, y0=cor.heivr - 1.96*cor.heivr.se,
                x1=N, y1=cor.heivr + 1.96*cor.heivr.se, 
                col=col["heivr"], lwd=2, code=1, angle=90, length=0))

with(df, arrows(x0=N, y0=cor.deming - 1.96*cor.deming.se,
                x1=N, y1=cor.deming + 1.96*cor.deming.se, 
                col=col["Deming"], lwd=2, code=1, angle=90, length=0))

# with(df, points(N, cov, pch=20))

with(df, points(N, cor, pch=20, col=col["Pearson"]))
with(df, points(N, cor.heivr, pch=20, col=col["heivr"]))
with(df, points(N, cor.deming, pch=20, col=col["Deming"]))
abline(h=cor.target, lty="dashed")
legend("bottomright", names(col), fill = col, title="Method", border="white", bty='n')
```





<!---
# fit = nls( estimate ~ SSlogis(N, a, b, c), data = as.data.frame(df), weights=1/df$se^2)
# lines(df$N, predict(fit), col="red", lwd=2) 
--->












